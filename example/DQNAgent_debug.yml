alias:
  width: &width 84
  height: &height 84
  stack: &stack 4
  batch: &batch 32

name: DQNAgent
args:
  recorder_config:
    memory_size: 1000000
    batch_size: *batch
    buffer_config:
      state:
        shape:
          - *height
          - *width
        dtype: uint8
        stack: *stack
        transition: 2
      action:
        dtype: uint8
      reward:
        dtype: float
      terminal:
        dtype: bool


  q_network_config:
    model_name: vanilla_dqn
    parameter_file: null
    state_width: *width
    state_height: *height
    state_length: *stack
    args:
      discount_rate: 0.99
      # Divide reward befor clipping
      scale_reward: 1.0
      # reward is clipped between the following min and max
      min_reward: -1.0
      max_reward: 1.0
      # error between predicted Q value and target Q value is clipped by the following min and max
      min_delta: -1.0
      max_delta: 1.0


  optimizer_config:
    name: NeonRMSProp
    args:
      decay: 0.95
      epsilon: 0.000001
      learning_rate: 0.00025

  action_config:
    # Exploration period in which exploration rate is annealed.
    annealing_method: linear
    exploration_period: 500
    initial_exploration_rate: 1.0
    terminal_exploration_rate: 0.1

  training_config:
    # Trainig starts after this number of frames are recorded
    train_start: 400

    # Train network every this number of observations are made
    train_frequency: 4

    # Sync networks every this number of observationa are made
    sync_frequency: 20
    n_samples: *batch

  save_config:
    # Save network parameter every once after this #trainings
    interval: 100
    saver_config:
      output_dir: &output_dir results
      max_to_keep: 10
      keep_every_n_hours: 1.0
      prefix: DQN

  summary_config:
    # Summarize network every once after this #trainings
    interval: 100
    writer_config:
      output_dir: *output_dir
