alias:
  width: &width 84
  height: &height 84
  stack: &stack 4
  batch: &batch 32
  save_dir: &save_dir results
  save_prefix: &save_prefix DQN_integration_test
  initial_parameter: &initial_parameter example/space_invaders_vanilla_dqn_99000.h5

name: DQNAgent
args:
  recorder_config:
    memory_size: 100
    batch_size: *batch
    buffer_config:
      state:
        shape:
          - *height
          - *width
        dtype: uint8
        stack: *stack
        transition: 2
      action:
        dtype: uint8
      reward:
        dtype: float
      terminal:
        dtype: bool

  q_network_config:
    model_config:
      name: example/vanilla_dqn.yml
      initial_parameter: *initial_parameter
      input_channel: *stack
      input_height: *height
      input_width: *width
    q_learning_config:
      discount_rate: 0.99
      # reward is clipped between the following min and max
      min_reward: -1.0
      max_reward: 1.0
    cost_config:
      name: SSE2
      args:
        # error between predicted Q value and target Q value is clipped by the following min and max
        min_delta: -1.0
        max_delta: 1.0
    optimizer_config:
      name: NeonRMSProp
      args:
        decay: 0.95
        epsilon: 0.000001
        learning_rate: 0.00025
    saver_config:
      output_dir: *save_dir
      max_to_keep: 10
      keep_every_n_hours: 1.0
      prefix: *save_prefix
    summary_writer_config:
      output_dir: *save_dir
    session_config:
      parameter_file: *initial_parameter

  save_config:
    # Save network parameter every once after this #trainings
    # Giving non-positive value effectively disable save functionality
    interval: -1

  summary_config:
    # Summarize network every once after this #trainings
    # Giving non-positive value effectively disable save functionality
    interval: -1

  action_config:
    method: linear
    duration: 0
    epsilon_init: 0.05
    epsilon_term: 0.05

  training_config:
    # Training starts after this number of transitions are recorded
    # Giving negative value effectively disable training and network sync
    train_start: -1
    # Train network every this number of observations are made
    train_frequency: 4
    # Sync networks every this number of observations are made
    sync_frequency: 10000
    n_samples: *batch
