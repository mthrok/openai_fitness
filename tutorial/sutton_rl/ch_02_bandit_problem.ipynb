{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Bandit Problems\n",
    "In this tutorial, we cover $n$-Armed Bandit Problem, following Chapter 2 of\n",
    "`Reinforcement Learning: An Introduction` to study fundamental properties\n",
    "of reinforcement learning, such as evaluative feedback and explotation vs\n",
    "exprolitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Bandit Problems\n",
    "\n",
    "The following is the definition/explanation of bandit problem from the book.\n",
    "\n",
    "---\n",
    "\n",
    "*You are faced repeatedly with a choice among $n$ different options, or *\n",
    "*actions. After each choice you receive a numerical reward chosen from a *\n",
    "*stationary probability distribution that depends on the action you *\n",
    "*selected. Your objective is to maximize the expected total reward over *\n",
    "*some time period, for example, over 1000 action selections, or time steps.*\n",
    "\n",
    "*This is the original form of the $n$-armed bandit problem, so named by *\n",
    "*analogy to a slot machine, or \"one-armed bandit,\" except that it has n *\n",
    "*levers instead of one. Each action selection is like a play of one of *\n",
    "*the slot machine's levers, and the rewards are the payoffs for hitting *\n",
    "*the jackpot. Through repeated action selections you are to maximize *\n",
    "*your winnings by concentrating your actions on the best levers.*\n",
    "\n",
    "---\n",
    "\n",
    "To summarize:\n",
    "- You have a set of possible actions.\n",
    "- At each time setp, you take action, you receive reward.\n",
    "- You can take an action for a certain number of times.\n",
    "- Your objective is to maximize *the sum of all rewards* you receive\n",
    "through all the trial.\n",
    "- Rewards are drawn from distributions correspond to taken actions.\n",
    "- The reward distributions are fixed, thus any action taken at any\n",
    "time do not have any effect on future rewards.\n",
    "- You do not know the reward distributions beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create bandit problem using by subclassing luchador base environment\n",
    "class, as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pylint: disable=invalid-name,attribute-defined-outside-init\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import luchador.env\n",
    "import luchador.agent\n",
    "\n",
    "import ch_02_bandit_problem_util as util\n",
    "\n",
    "\n",
    "class Bandit(luchador.env.BaseEnvironment):\n",
    "    \"\"\"N-armed bandit problem\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        The number of arms of bandit\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, seed=None):\n",
    "        self.n_arms = n_arms\n",
    "        self.rng = np.random.RandomState(seed=seed)\n",
    "\n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return self.n_arms\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Set distribution mean randomly\"\"\"\n",
    "        self.mean = [self.rng.randn() for _ in range(self.n_arms)]\n",
    "        self.stddev = [1 for _ in range(self.n_arms)]\n",
    "        return luchador.env.Outcome(\n",
    "            reward=0, observation=None, terminal=False)\n",
    "\n",
    "    def step(self, n):\n",
    "        \"\"\"Sample reward from the given distribution\"\"\"\n",
    "        reward = self.rng.normal(loc=self.mean[n], scale=self.stddev[n])\n",
    "        return luchador.env.Outcome(\n",
    "            reward=reward, observation=None, terminal=False)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(\n",
    "            'Acttion {:3d}: Mean {:5.2f}, Stddev {:5.2f}'.format(\n",
    "                i, self.mean[i], self.stddev[i]) for i in range(self.n_arms)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to initialize reward distribution as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bandit = Bandit(n_arms=10, seed=10)\n",
    "# `reset` generates rewards distributions with means randomly drawn\n",
    "# from normal distribution and variance=1\n",
    "bandit.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can peak in the resulting distributions as following.\n",
    "This is not know to agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean values shown above are called `value` of each action. If agents\n",
    "knew these values, then they could solve bandit problem just by selecting\n",
    "the action with the highest value. Agents, however can only estimate these\n",
    "value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `step` method to take an action over the environment.\n",
    "The argument to the method in this case is the index of distribution from\n",
    "which reward is drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for action in range(bandit.n_actions):\n",
    "    outcome = bandit.step(action)\n",
    "    print(action, outcome.reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploitation, Exploration and Greedy Action\n",
    "\n",
    "Let's estimate action values by taking average rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We take each action 3 times just to have estimates for all actions for\n",
    "# illustration purpose.\n",
    "n_trials = 3\n",
    "print('Hypothetical Estimation:')\n",
    "for i in range(bandit.n_actions):\n",
    "    value = sum([bandit.step(n=i).reward for _ in range(n_trials)]) / n_trials\n",
    "    print('Action {}: {:7.3f}'.format(i, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that these estimates are not accurate, but agent does not know\n",
    "the true action values. The action with the highest estimated value is called\n",
    "*greedy action*. When making decision on which action to take next,\n",
    "agent can eiether repeat the greedy action to maximize total rewards, or\n",
    "try other actions. The former is called *exploitation*, and the latter\n",
    "is called *exploration*. Since exploitation and exploration cannot be carried\n",
    "out at the same time, agents have way to balance actions between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-value method and epsilon-greedy policy\n",
    "\n",
    "Let's incorperate some action selection into simple average action value\n",
    "gestimation.\n",
    "\n",
    "We consider the following rules.\n",
    "1. Select the actions with highest estimated action values at the time being.\n",
    "2. Behave like 1 most of the time, but every once in a while, select action\n",
    "randomly with equal probability.\n",
    "\n",
    "Rule 2. is called epsilon-greedy method, where epsilon represents the\n",
    "probability of taking random action. Rule 1. is called greedy method but can\n",
    "be considered as a special case of epsilon-greedy method, that is epsilon=0.\n",
    "\n",
    "To see the different behavior of these rules, let's run an experiment.\n",
    "In this experiment, we run 2000 independant 10-armed bandit problem.\n",
    "\n",
    "Agents estimate action values by tracking average rewards for each action\n",
    "and select the next action based on epsilon-greedy method.\n",
    "Such agent can be implemented as following, using base luchador agent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EGreedyAgent(luchador.agent.BaseAgent):\n",
    "    \"\"\"Simple E-Greedy policy for stationary environment\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epsolon : float\n",
    "        The probability to take random action.\n",
    "\n",
    "    step_size : 'average' or float\n",
    "        Parameter to adjust how action value is estimated from the series of\n",
    "        observations. When 'average', estimated action value is simply the mean\n",
    "        of all the observed rewards for the action. When float, estimation is\n",
    "        updated with weighted sum over current estimation and newly observed\n",
    "        reward value.\n",
    "\n",
    "    initial_q : float\n",
    "        Initial Q value for all actions\n",
    "\n",
    "    seed : int\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon, step_size='average', initial_q=0.0, seed=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.step_size = step_size\n",
    "        self.initial_q = initial_q\n",
    "        self.rng = np.random.RandomState(seed=seed)\n",
    "\n",
    "    def reset(self, observation):\n",
    "        self.q_values = [self.initial_q] * self.n_actions\n",
    "        self.n_trials = [self.initial_q] * self.n_actions\n",
    "\n",
    "    def init(self, environment):\n",
    "        self.n_actions = environment.n_actions\n",
    "\n",
    "    def observe(self, action, outcome):\n",
    "        \"\"\"Update the action value estimation based on observed outcome\"\"\"\n",
    "        r, n, q = outcome.reward, self.n_trials[action], self.q_values[action]\n",
    "        alpha = 1 / (n + 1) if self.step_size == 'average' else self.step_size\n",
    "        self.q_values[action] += (r - q) * alpha\n",
    "        self.n_trials[action] += 1\n",
    "\n",
    "    def act(self, _=None):\n",
    "        \"\"\"Choose action based on e-greedy policy\"\"\"\n",
    "        if self.rng.rand() < self.epsilon:\n",
    "            return self.rng.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10-armed test bed for with different epsilon-greedy method, and\n",
    "plot rewards and the number of optimal actions taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilons = [0.0, 0.01, 0.1]\n",
    "mean_rewards, optimal_actions = [], []\n",
    "for eps in epsilons:\n",
    "    print('Running epsilon = {}...'.format(eps))\n",
    "    env = Bandit(n_arms=10, seed=0)\n",
    "    agent = EGreedyAgent(epsilon=eps)\n",
    "    agent.init(env)\n",
    "    rewards, actions = util.run_episodes(env, agent, episodes=2000, steps=1000)\n",
    "    mean_rewards.append(rewards)\n",
    "    optimal_actions.append(actions)\n",
    "\n",
    "util.plot_result(epsilons, mean_rewards, optimal_actions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imcremental Implementation to non-stationary extention\n",
    "$Q_t$, estimated value of an action at time step $t$, as mean\n",
    "observed reward can be written in recursive manner as follow\n",
    "\n",
    "\\begin{align}\n",
    "Q_t &= \\frac{R_1 +R_2 + \\dots +R_{K_a}}{K_a} \\\\\n",
    "    &= \\frac{1}{K_a} \\sum_{i=1}^{K_a}R_{i} \\\\\n",
    "    &= \\frac{1}{K_a} ( R_{K_a} + \\sum_{i=1}^{K_a - 1}R_{i} ) \\\\\n",
    "    &= \\frac{1}{K_a} \\left\\{ R_{K_a} + ( K_a - 1 )\n",
    "           \\frac{1}{K_a - 1}\\sum_{i=1}^{K_a - 1}R_{i} \\right\\} \\\\\n",
    "    &= \\frac{1}{K_a} \\left\\{ R_{K_a} + ( K_a - 1 ) Q_{t-1} \\right\\} \\\\\n",
    "    &= Q_{t-1} + \\frac{1}{K_a} ( R_{K_a} - Q_{t-1} )\n",
    "\\end{align}\n",
    "\n",
    "This can be generalized iterative update form\n",
    "$$ NewEstimate \\leftarrow OldEstimate + StepSize ( Target - OldEstimate ) $$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}