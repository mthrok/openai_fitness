{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Tutorial for RL introduction, following Chapter 3 of Sutton\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Reinforcement Learning Problem\n",
    "In this tutorial we go through the example of computing action-value\n",
    "function and state-value function following the example given in Chapter 3\n",
    "of `Reinforcement Learning: An Introduction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State value function and action value function for policy $\\pi$ are defined\n",
    "as follow\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi} (s)\n",
    "  &= \\mathbb{E}_{\\pi} \\lbrack G_t | S_t = s \\rbrack \\\\\n",
    "q_{\\pi}(s, a)\n",
    "  &= \\mathbb{E}_{\\pi} \\lbrack G_t | S_t = s, A_t = a \\rbrack \\\\\n",
    "\\text{where } & \\text{$G_t$ , the sum of the discounted rewards, is} \\\\\n",
    "G_t\n",
    "  &= R_{t+1} + R_{t+2} + R_{t+3} + \\dots + R_{T}\n",
    "\\end{align*}\n",
    "\n",
    "In case of Markov Decision Process, the following can be derived.\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi} (s)\n",
    "  &= \\mathbb{E}_{\\pi} \\lbrack G_t | S_t = s \\rbrack \\\\\n",
    "  &= \\mathbb{E}_{\\pi} \\lbrack \\sum_{k=0}^{\\infty}\n",
    "     \\gamma ^ {k} R_{t+k+1} | S_t = s \\rbrack \\\\\n",
    "q_{\\pi}(s, a)\n",
    "  &= \\mathbb{E}_{\\pi} \\lbrack G_t | S_t = s, A_t = a \\rbrack \\\\\n",
    "  &= \\mathbb{E}_{\\pi} \\lbrack \\sum_{k=0}^{\\infty}\n",
    "     \\gamma ^ {k} R_{t+k+1} | S_t = s, A_t = a \\rbrack\n",
    "\\end{align*}\n",
    "\n",
    "These interweived functions statisfy recursive relationships as follow\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi} (s)\n",
    "  &= \\mathbb{E}_{\\pi} \\lbrack G_t | S_t = s \\rbrack \\\\\n",
    "  &= \\mathbb{E}_{\\pi} \\lbrack\n",
    "     \\sum_{k=0}^{\\infty} \\gamma ^ k R_{t+k+1} | S_t = s \\rbrack \\\\\n",
    "  &= \\mathbb{E}_{\\pi} \\lbrack\n",
    "     R_{t+1} + \\gamma\n",
    "     \\sum_{k=0}^{\\infty} \\gamma ^ k R_{t+k+2} | S_t = s \\rbrack \\\\\n",
    "  &= \\sum_a \\pi(a|s) \\sum_{s'} p(s' |s, a) \\lbrack\n",
    "     r(s, a, s') + \\gamma \\mathbb{E}_{\\pi} \\lbrack\n",
    "         \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+2} | S_{t+1} = s'\n",
    "       \\rbrack\n",
    "     \\rbrack \\\\\n",
    "  &= \\sum_a \\pi(a|s) \\sum_{s'} p(s' |s, a) \\lbrack\n",
    "     r(s, a, s') + \\gamma v_{\\pi}(s')\n",
    "     \\rbrack\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute and visualize the state-value function through GridWorld\n",
    "example.\n",
    "For the detail of the definition, please refer to the Example 3.8\n",
    "from the book.\n",
    "- Agent moves in 5 x 5 grid cell\n",
    "- Agent takes action to move either north, east, west, or south.\n",
    "- Actions that would move the agent out of grid results in reward of -1\n",
    "and agent stays where it was before taking the action.\n",
    "- On success full action, agent moves to new position and receives\n",
    "0 reward.\n",
    "- In special state $A$ (0, 1), all action cause the agent to move to $A'$\n",
    "(4, 1) and reward of 10\n",
    "- Similarly in $B$ (0, 3), all action cause the agent to move to $B'$\n",
    "(2, 3) and reward of 5\n",
    "\n",
    "First, we define `GridWorld` environment as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import luchador.env\n",
    "import luchador.agent\n",
    "from luchador.episode_runner import EpisodeRunner\n",
    "\n",
    "\n",
    "def _transit(position, action):\n",
    "    \"\"\"Transition rule of GridWorld\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    position : NumPy NDArray\n",
    "        Coordinate of agent\n",
    "    action : int\n",
    "        0, 1, 2, or 3, meaning north, east, west or south respectively\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    NumPy NDArray\n",
    "        New coordinate\n",
    "    int\n",
    "        Reward\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    new_position = position.copy()\n",
    "\n",
    "    if np.all(position == [0, 1]):\n",
    "        reward = 10\n",
    "        new_position[:] = [4, 1]\n",
    "        return new_position, reward\n",
    "    if np.all(position == [0, 3]):\n",
    "        reward = 5\n",
    "        new_position[:] = [2, 3]\n",
    "        return new_position, reward\n",
    "\n",
    "    if action == 0:  # North\n",
    "        move = [-1, 0]\n",
    "    elif action == 1:  # East\n",
    "        move = [0, 1]\n",
    "    elif action == 2:  # West\n",
    "        move = [0, -1]\n",
    "    elif action == 3:  # South\n",
    "        move = [1, 0]\n",
    "\n",
    "    new_position = new_position + move\n",
    "    if np.any(new_position < 0) or np.any(new_position > 4):\n",
    "        reward = -1\n",
    "        new_position[new_position < 0] = 0\n",
    "        new_position[new_position > 4] = 4\n",
    "    return new_position, reward\n",
    "\n",
    "\n",
    "class GridWorld(luchador.env.BaseEnvironment):\n",
    "    \"\"\"GridWorld Example from Sutton, Chapter3.\"\"\"\n",
    "    def __init__(self, seed=None):\n",
    "        self.position = None\n",
    "        self.rng = np.random.RandomState(seed=seed)\n",
    "\n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return 4\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset position randomly\"\"\"\n",
    "        self.position = self.rng.randint(5, size=2)\n",
    "        return luchador.env.Outcome(\n",
    "            reward=0, observation=self.position, terminal=False, state={})\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Move position based on action and transit rule\"\"\"\n",
    "        self.position, reward = _transit(self.position, action)\n",
    "        return luchador.env.Outcome(\n",
    "            reward=reward, observation=self.position, terminal=False, state={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create agent which\n",
    "- has equiprobable random policy\n",
    "- estimates of action-value function via Monte-Calro approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GridWorldAgent(luchador.agent.BaseAgent):\n",
    "    \"\"\"Agent walk on GridWorld with equiprobable random policy while\n",
    "    estimating the action values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    step_size : float\n",
    "        StepSize parameter for estimating action value function\n",
    "    discount : float\n",
    "        Discount rate for computing state-value function\n",
    "    initial_q : float\n",
    "        Initial action value estimation\n",
    "    \"\"\"\n",
    "    def __init__(self, step_size=0.9, discount=0.9, initial_q=10):\n",
    "        self.step_size = step_size\n",
    "        self.discount = discount\n",
    "\n",
    "        self.position = None  # Pre-action position\n",
    "        self.action_values = initial_q * np.ones((5, 5, 4))\n",
    "\n",
    "    @property\n",
    "    def state_values(self):\n",
    "        \"\"\"Current estimated state value mapping\"\"\"\n",
    "        return np.mean(self.action_values, axis=2)\n",
    "\n",
    "    def init(self, _):\n",
    "        pass\n",
    "\n",
    "    def reset(self, observation):\n",
    "        self.position = observation\n",
    "\n",
    "    def observe(self, action, outcome):\n",
    "        pos0, pos1 = self.position, outcome.observation\n",
    "\n",
    "        post_state_value = self.state_values[pos1[0], pos1[1]]\n",
    "        target = outcome.reward + self.discount * post_state_value\n",
    "\n",
    "        self.action_values[pos0[0], pos0[1], action] += self.step_size * (\n",
    "            target - self.action_values[pos0[0], pos0[1], action])\n",
    "        self.position = pos1\n",
    "\n",
    "    def act(self):\n",
    "        return np.random.choice(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the agent in the environment for setps long enough for action value\n",
    "estimation to get close enough to theoritical value as given in the book,\n",
    "Fig.3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_agent(env, agent, episodes=1000, steps=4):\n",
    "    \"\"\"Run agent for the given steps and plot the resulting state value\"\"\"\n",
    "    runner = EpisodeRunner(env, agent, max_steps=steps)\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        runner.run_episode()\n",
    "\n",
    "    print('State Value:\\n', agent.state_values)\n",
    "    for i, action in enumerate(['north', 'east', 'west', 'south']):\n",
    "        print('Action Value:', action)\n",
    "        print(agent.action_values[:, :, i])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    img = ax.imshow(agent.state_values, interpolation='nearest', origin='upper')\n",
    "    fig.colorbar(img)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "run_agent(\n",
    "    env=GridWorld(seed=0),\n",
    "    agent=GridWorldAgent(step_size=0.9, discount=0.9),\n",
    "    steps=5, episodes=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the state values for optimal policy. $\\pi^{*}$\n",
    "Computing optimal policy using Monte Carlo (sampling) approach is not\n",
    "straight forward, as agent has to explore the transitions which are not\n",
    "optimal.\n",
    "To overcome this, we take advantage of\n",
    "- random initialization: All states are visited eventually\n",
    "- Optimal initial value: All actions are tried eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GreedyGridWorldAgent(GridWorldAgent):\n",
    "    \"\"\"Agent act on greedy policy\"\"\"\n",
    "    def __init__(self, step_size=0.9, discount=0.9, initial_q=30):\n",
    "        super(GreedyGridWorldAgent, self).__init__(\n",
    "            step_size=step_size, discount=discount, initial_q=initial_q)\n",
    "\n",
    "    @property\n",
    "    def state_values(self):\n",
    "        return np.max(self.action_values, axis=2)\n",
    "\n",
    "    def act(self):\n",
    "        return np.argmax(\n",
    "            self.action_values[self.position[0], self.position[1]])\n",
    "\n",
    "\n",
    "run_agent(\n",
    "    env=GridWorld(seed=0),\n",
    "    agent=GreedyGridWorldAgent(step_size=0.9, discount=0.9, initial_q=30),\n",
    "    steps=100, episodes=100,\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}